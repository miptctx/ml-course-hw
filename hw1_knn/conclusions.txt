ВЫВОДЫ
------

В ходе данной практической работы построил несколько классов классификатора.

Первый класс был расширением уже существующего класса классификатора. В нем учитывался вес каждого соседа в зависимости от расстояния. Поскольку данный класс почти полностью повторял функционал базового класса, то работал он тоже очень медлеенно.

Далее я немного улучшил этот класс с помощью бродкастинга избавившись т.о. от одного цикла (при подсчете расстояний), что немного ускорило работу классификатора.

Однако по прежнему скорость работы этого классификатора была довольно медленная, поэтому я использовал фукнцию cdist пакета scipy которая помогла мне получить все расстояния между элементами тестовой выборки в виде матрицы. Это позволило мне в начале прграммы получить матрицу расстояний, после чего далее работать уже с этой матрицей. Это значительно ускорило время выполнения программы.

В конце мне удалось еще немного улучшить классификатор т.к. я нашел функцию numpy.argsort, которая возвращает индексы элементов если бы они были отсортированы. Это дало мне возможность избавиться от лишней операции транспонирования, что еще лучше отразилось на скорости работы алгоритма.

Последний класс срабатывал уже быстрее классификатора из пакета sklearn и выдавал лучшее значение accuracy, которое достигало значения 0.974 против 0.966 sklearn-овского.

АНАЛИЗ ОШИБОК
-------------

Как ошибки связаны с расстоянием до других объектов класса?

Может оказаться так, что объект будет окружать одинаковое число соседей разных классов. Если существует всего два класса, то можно взять нечетное число соседей. Но лучше использовать веса, которые уменьшают вклад соседа обратно пропорционально расстоянию до него. Если зависимость веса от расстояния линейная, то неоднозначности все еще могут возникать, например, если объект окружают 4 соседа, сосед с номером 1 и 4 принадлежат классу А, а соседи 2 и 3 - классу Б, в сумме получается одинаковый вес для обоих классов. Чтобы разрешить эту неоднозначность, применяют нелинейную убывающую последовательность весов.


Какая реальная зависимость точности от гиперпараметра k_neighbours? Проведите анализ -- почему она такая?

Как показал опыт, существует оптимальное значение этого параметра, когда модель выдает наиболее точный результат. Повидимому, это связано с тем, что если мы выбираем значение этого параметра маленьким, то модель начинает принимать неверные решение как раз таки из-за того, что у нее маленькое количество примеров что и приводит к увеличению ошибки, а также это заставляет модель реагировать на шумы. Если же значение этого параметра большое, то происходит, в принятии решения учавствует большое число классов из-за чего стрираются локальные особенности объектов, что и приводит к очень грубой оценке.


Почему реализация "из коробки" отличается от нашей реализации?

Возможно, это связано с тем, что мы писали перебор соседей с помощью питоновских конструкций и циклов, в то время как реализация склерн использует более низкоуровневый интерфейс перебора и поиска сосдей. Также склерн может распарраллеливать свои вычисления.
